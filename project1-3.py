# -*- coding: utf-8 -*-
"""project1.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1_ltiS57ocfBxHO4dY85u6er95DmqWc5V
"""

import requests as rq
from bs4 import BeautifulSoup

import nltk
from nltk.tokenize import word_tokenize, wordpunct_tokenize
from nltk import download
download('punkt')
download('stopwords')
from nltk.corpus import stopwords
stop_words = stopwords.words('russian') 
!pip install pymorphy2
from pymorphy2 import MorphAnalyzer
morph = MorphAnalyzer()

url = 'https://libreed.ru/books/moe-stoletie'
page = rq.get(url) 
print(page)

soup = BeautifulSoup(page.text, features="html.parser") 
print(soup.prettify())

text1 = []
for i in soup.find_all("p"):
  text1.append(i.text)
text1 [0: -1]

text1_str = (''.join(text1[0: -1]))
text1_str

all_links = ['https://libreed.ru/books/moe-stoletie?page_book=2']
for i in range(2, 196):
  link = 'https://libreed.ru/books/moe-stoletie?page_book=' + str(i)
  all_links.append(link)
len(all_links)

all_links

text = []
for i in all_links:
  url = i
  page = rq.get(url) 
  soup = BeautifulSoup(page.text, features="html.parser") 
  for sent in soup.find_all("p")[0:-1]:
    text.append(sent.text)

text

text_str = ' '.join(text)
text_str

text_full = [text1_str, text_str]
text_full

text_full_str = ' '.join(text_full)
text_full_str = text_full_str.replace('\xa0', ' ')
text_full_str

def clean_text(text_full_str):
  text_full_str = text_full_str.lower() 
  text_list_nltk = word_tokenize(text_full_str) 
  stop_words = stopwords.words('russian') 
  text_without_punkt = [word for word in text_list_nltk if word[0].isalpha()] 
  text_clean = [word for word in text_without_punkt if word not in stop_words] 
  return text_clean

full_text_new = clean_text(text_full_str)
full_text_new

text_lemmatized = []
for word in full_text_new:
    result = morph.parse(word)
    most_probable_result = result[0] 
    normal_form = most_probable_result.normal_form
    text_lemmatized.append(normal_form)

text_lemmatized

from nltk import Text as nltk_text

text = nltk_text(text_full_str.split())
text.dispersion_plot(['тяжелые', 'война', 'время', 'память'])

from sklearn.feature_extraction.text import TfidfVectorizer
tfidf_vectorizer = TfidfVectorizer()

tfidf = tfidf_vectorizer.fit_transform(text_lemmatized) # считаем частоту 
print(tfidf)

tfidf.shape

tfidf.todense()

tfidf_vectorizer.vocabulary_

words = tfidf_vectorizer.get_feature_names_out()
words

import pandas as pd

data = tfidf.todense().tolist() 
lotr = pd.DataFrame(data, columns = words)
lotr

!pip install stanza
import stanza
stanza.download('ru')
def stanza_nlp_ru(text):
  nlp = stanza.Pipeline(lang='ru', processors='tokenize,ner')
  doc = nlp(text_str )
  print(*[f'entity: {ent.text}\ttype: {ent.type}' for sent in doc.sentences for ent in sent.ents], sep='\n')
stanza_nlp_ru(text_str)

!pip install -q deeppavlov

!python -m deeppavlov install ner_ontonotes_bert

from deeppavlov import build_model

ner_model = build_model('ner_ontonotes_bert', download=True, install=True)

ner_model(text1_str)